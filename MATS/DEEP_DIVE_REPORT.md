# MATS 10.0: Deep Dive Results

## Causal Evidence for Sycophancy Entropy Control

**Author:** Ajay S Patil  
**Date:** December 27, 2025  
**Model:** Qwen/Qwen2.5-7B-Instruct (28L Ã— 28H, GQA 7:1)

---

## Executive Summary

**ðŸ† CAUSAL FLIP ACHIEVED**

We found that **sharpening attention head L23H5 with Î±=2.0** causes the model to **flip from sycophantic to correct reasoning** under adversarial prompts. This provides causal evidence for the "Logic Head" hypothesis.

---

## 1. Key Finding: Generated-Tokens-Only Entropy

The original sanity check failed (max Î”E = 0.226) because it measured entropy on the **entire sequence including prompt tokens**. Prompt tokens are static and dilute the signal.

When measuring **only on generated (CoT) tokens**:

| Layer  | Head  | Î”Entropy   | Classification                                |
| ------ | ----- | ---------- | --------------------------------------------- |
| **23** | **5** | **+1.162** | ðŸ”¥ **LOGIC HEAD** (diffuses under sycophancy) |
| 24     | 4     | +0.252     | Logic Head candidate                          |
| 23     | 4     | +0.192     | Logic Head candidate                          |
| 23     | 15    | -0.473     | Sycophancy Head candidate                     |
| **27** | **5** | **-0.630** | â„ï¸ **SYCOPHANCY HEAD** (sharpens on hint)     |

**Interpretation:**

- **L23H5** shows +1.162 Î”E â†’ It becomes **diffuse** when rationalizing (struggling to bridge truth and lie)
- **L27H5** shows -0.630 Î”E â†’ It becomes **sharp** when sycophantic (locking onto hint)

---

## 2. Stimulus Validation

The model (Qwen2.5-7B-Instruct) **resisted weak sycophancy prompts**:

| Prompt Type                       | Model Behavior                             |
| --------------------------------- | ------------------------------------------ |
| Control                           | Correct reasoning                          |
| Weak ("Student says 26")          | Corrected the student                      |
| Strong ("Professor says 26")      | Corrected the professor                    |
| Very Strong ("Confirm my answer") | **Sycophantic** (forced agreement framing) |

**Conclusion:** Qwen2.5-7B is well-aligned and requires strong adversarial prompts to exhibit sycophancy. The "Very Strong" prompt successfully induced the target behavior.

---

## 3. Causal Intervention: The Critical Test

**Hypothesis:** If L23H5 is a "Logic Head" that blurs during rationalization, **sharpening it (Î± > 1)** should restore correct reasoning.

### Protocol

1. Use "Very Strong" sycophancy prompt (forces model to agree with wrong answer)
2. **Baseline:** No intervention â†’ Model outputs path toward wrong answer
3. **Intervention:** Sharpen L23H5 (Î±=2.0) â†’ Observe if output changes

### Results

**Baseline Output (sycophantic):**

```
[16 - 7 = 9 eggs, but continues with sycophantic framing...]
Baseline sycophantic: True
```

**Intervention Output (L23H5 Î±=2.0):**

```
"Therefore, the amount of money she makes from selling the eggs is 9 eggs * $2/egg = $18."
```

**ðŸ† CAUSAL FLIP DETECTED**

Sharpening L23H5 **restored the correct computation path**. The model computed the same intermediate value (9 eggs) but then correctly multiplied by $2 to get **$18**.

---

## 4. Attention Target Analysis

L23H5's attention from the last generated token:

| Position | Weight | Token   |
| -------- | ------ | ------- |
| 2        | 0.414  | "Janet" |
| 97       | 0.128  | "You"   |
| 64       | 0.063  | "User"  |

**Interpretation:** L23H5 attends strongly to the **subject of the problem** ("Janet"), which may represent the "anchor" for correct reasoning. When this head blurs, it loses this anchor.

---

## 5. Mechanistic Hypothesis (Updated)

```
Sycophancy Prompt
       â†“
L23H5 becomes DIFFUSE (Î”E = +1.16)
  â†’ Loses anchor on problem subject
  â†’ Struggles to compute correct answer
       â†“
L27H5 becomes SHARP (Î”E = -0.63)
  â†’ Locks onto user's hint
  â†’ Biases output toward wrong answer
       â†“
INTERVENTION: Sharpen L23H5 (Î±=2.0)
  â†’ Restores focus on problem
  â†’ Correct answer emerges
```

---

## 6. Statistical Summary

| Metric                | Sanity Check (All Tokens) | Deep Dive (Generated Only) |
| --------------------- | ------------------------- | -------------------------- | ----- | --------- |
| Max                   | Î”E                        |                            | 0.226 | **1.162** |
| Heads > 0.3 threshold | 0                         | **4**                      |
| Causal flip           | Not tested                | **YES**                    |

---

## 7. Significance

This result provides:

1. **Causal evidence** (not just correlation) that attention entropy affects sycophancy
2. **Identification** of a specific "Logic Head" (L23H5) and "Sycophancy Head" (L27H5)
3. **Intervention method** that reduces sycophancy without prompt modification
4. **Methodological lesson:** Measure entropy on generated tokens only

---

## 8. Next Steps

1. **Validate on multiple problems** - Does L23H5 sharpening work generally?
2. **Test L27H5 flattening** - Does blurring the Sycophancy Head also help?
3. **Alpha sweep** - Find optimal Î± value
4. **Other models** - Does this transfer to LLaMA, Mistral?

---

## Appendix: Reproducibility

- **Git Commit:** 4f6b3ba
- **Script:** `python deep_dive.py`
- **Seed:** 42
- **Results:** `results/deep_dive/`
