{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"],"fields":{"title":{"boost":1000.0},"text":{"boost":1.0},"tags":{"boost":1000000.0}}},"docs":[{"location":"","title":"Oculi Documentation","text":"Comprehensive Mechanistic Interpretability Toolkit for Transformer LLMs    [![Version](https://img.shields.io/badge/version-0.3.0--dev-blue)]()   [![Python](https://img.shields.io/badge/python-3.10+-green)]()   [![License](https://img.shields.io/badge/license-MIT-lightgrey)]()   [![Tests](https://img.shields.io/badge/tests-85%20passing-brightgreen)]()"},{"location":"#welcome-to-oculi","title":"Welcome to Oculi","text":"<p>Oculi is a research-first mechanistic interpretability toolkit for transformer language models. It provides surgical instrumentation for understanding how transformers work internally.</p> <p>What makes Oculi different?</p> <ul> <li>Learning-First Design \u2014 Adapters are executable documentation</li> <li>Explicit Control \u2014 No magic, you choose what to capture</li> <li>Pure Functional \u2014 Stateless, deterministic analysis</li> <li>Memory-Conscious \u2014 Selective capture, efficient storage</li> </ul>"},{"location":"#features","title":"Features","text":""},{"location":"#comprehensive-capture-system","title":"\u2705 Comprehensive Capture System","text":"<ul> <li>Attention Internals \u2014 Q/K/V vectors, attention patterns with pre/post-RoPE options</li> <li>Residual Stream \u2014 Activations at all intervention points (pre/post attention, pre/post MLP)</li> <li>MLP Internals \u2014 Gate, up projections, activations, and outputs</li> <li>Layer-wise Logits \u2014 Logit lens analysis with memory-efficient top-k</li> </ul>"},{"location":"#analysis-tools","title":"\ud83d\udd0d Analysis Tools","text":"<ul> <li>Circuit Detection \u2014 Automatic detection of induction heads, previous token heads</li> <li>Logit Lens \u2014 Track prediction formation across layers</li> <li>Attribution Methods \u2014 Understand information flow and component contributions \u2728 NEW</li> <li>Composition Analysis \u2014 Analyze how attention heads interact \u2728 NEW</li> <li>Entropy &amp; Norms \u2014 Attention focus metrics, vector magnitudes</li> <li>Correlation Analysis \u2014 Statistical relationships with p-values</li> </ul>"},{"location":"#surgical-interventions","title":"\ud83c\udfaf Surgical Interventions","text":"<ul> <li>Q/K Scaling \u2014 The Spectra method for attention sharpening/flattening</li> <li>Head Ablation \u2014 Zero out specific attention heads</li> <li>Activation Patching \u2014 (Coming in v0.6.0)</li> </ul>"},{"location":"#quick-example","title":"Quick Example","text":"<pre><code>from transformers import AutoModelForCausalLM, AutoTokenizer\nfrom oculi.models.llama import LlamaAttentionAdapter\nfrom oculi.analysis import AttributionMethods, CompositionAnalysis\n\n# Load model\nmodel = AutoModelForCausalLM.from_pretrained(\"meta-llama/Meta-Llama-3-8B\")\ntokenizer = AutoTokenizer.from_pretrained(\"meta-llama/Meta-Llama-3-8B\")\n\n# Create adapter\nadapter = LlamaAttentionAdapter(model, tokenizer)\n\n# Capture everything\ninput_ids = tokenizer.encode(\"The cat sat on the mat\", return_tensors=\"pt\")\nfull = adapter.capture_full(input_ids)\n\n# Analyze attribution\ntarget_token = tokenizer.encode(\"mat\")[0]\nattribution = AttributionMethods.direct_logit_attribution(\n    full.residual, model.lm_head.weight, target_token\n)\nprint(f\"Most important layer: {attribution.values.argmax()}\")\n\n# Detect induction circuits\ncircuits = CompositionAnalysis.detect_induction_circuit(full.attention)\nprint(f\"Found {len(circuits.metadata['circuits'])} circuits\")\n</code></pre>"},{"location":"#getting-started","title":"Getting Started","text":"<ul> <li> <p> Quick Start</p> <p>Get up and running in 5 minutes</p> <p> Quick Start</p> </li> <li> <p> User Guide</p> <p>In-depth guides for each feature</p> <p> Guides</p> </li> <li> <p> Tutorials</p> <p>Step-by-step tutorials with examples</p> <p> Tutorials</p> </li> <li> <p> API Reference</p> <p>Complete API documentation</p> <p> Reference</p> </li> </ul>"},{"location":"#latest-updates","title":"Latest Updates","text":""},{"location":"#phase-2-v050-dev-current","title":"Phase 2 (v0.5.0-dev) - Current","text":"\u2705 Attribution Methods <ul> <li>Attention flow tracking across layers</li> <li>Value-weighted attention patterns</li> <li>Direct logit attribution</li> <li>Component attribution (attention vs MLP)</li> <li>Head-level attribution</li> <li>Top-k attribution extraction</li> </ul> \u2705 Head Composition Analysis <ul> <li>QK composition between head pairs</li> <li>OV composition for value flow</li> <li>Virtual attention through multi-head paths</li> <li>Path patching importance scores</li> <li>Full composition matrices</li> <li>Automatic induction circuit detection</li> </ul> \ud83d\udd04 In Progress <ul> <li>Activation patching for causal interventions</li> <li>SAE integration</li> <li>Probing &amp; steering vectors</li> </ul>"},{"location":"#phase-1-v030-v040-complete","title":"Phase 1 (v0.3.0-v0.4.0) - Complete","text":"Completed Features <ul> <li>\u2705 Residual stream capture at all intervention points</li> <li>\u2705 MLP internals capture</li> <li>\u2705 Logit lens analysis</li> <li>\u2705 Circuit detection primitives</li> <li>\u2705 Unified full capture</li> </ul>"},{"location":"#supported-models","title":"Supported Models","text":"Model Family Adapter Attention Type Status LLaMA 2/3 <code>LlamaAttentionAdapter</code> GQA \u2705 Mistral Coming soon GQA \ud83d\udd04 Qwen 2/2.5 Coming soon GQA \ud83d\udd04"},{"location":"#installation","title":"Installation","text":"<pre><code># Basic installation\npip install oculi\n\n# From source (for development)\ngit clone https://github.com/ajayspatil7/oculi.git\ncd oculi\npip install -e .\n\n# With visualization support\npip install -e \".[viz]\"\n\n# With documentation tools\npip install -e \".[docs]\"\n\n# Everything\npip install -e \".[all]\"\n</code></pre> <p>Requirements: Python 3.10+, PyTorch 2.0.0+, Transformers 4.30.0+</p>"},{"location":"#community-support","title":"Community &amp; Support","text":"<ul> <li>GitHub Issues \u2014 Report bugs or request features</li> <li>Discussions \u2014 Ask questions and share ideas</li> <li>Contributing \u2014 Read our contributing guide</li> </ul>"},{"location":"#citation","title":"Citation","text":"<p>If you use Oculi in your research, please cite:</p> <pre><code>@software{oculi2024,\n  author = {Patil, Ajay S},\n  title = {Oculi: Mechanistic Interpretability Toolkit for Transformers},\n  year = {2024},\n  url = {https://github.com/ajayspatil7/oculi}\n}\n</code></pre>"},{"location":"#license","title":"License","text":"<p>MIT License - see LICENSE for details.</p> <sub>Built with \u2764\ufe0f for the mechanistic interpretability community</sub>"},{"location":"API_CONTRACT/","title":"Oculi API Contract Specification","text":"<p>Version: 0.1.0-draft Status: Pre-release (API may change before 1.0) Last Updated: 2026-01-01</p>"},{"location":"API_CONTRACT/#1-design-philosophy","title":"1. Design Philosophy","text":""},{"location":"API_CONTRACT/#11-abstraction-firewall","title":"1.1 Abstraction Firewall","text":"<p>Oculi maintains a strict separation between:</p> Layer Purpose Location Stability Public Research semantics <code>oculi/</code> Versioned, stable Private Engineering mechanics <code>oculi/_private/</code> Internal, may change <p>Guarantees:</p> <ul> <li>Public API changes trigger semantic versioning bumps</li> <li>Private implementation can be refactored without public API changes</li> <li>No PyTorch hooks, model-specific code, or implementation details in public layer</li> </ul>"},{"location":"API_CONTRACT/#12-guiding-principles","title":"1.2 Guiding Principles","text":"<ol> <li>Research Semantics First \u2014 APIs express what researchers reason about</li> <li>Deterministic by Default \u2014 Same inputs \u2192 same outputs (within floating-point limits)</li> <li>Inference Only \u2014 No training, no gradient flows through captures</li> <li>Pure Functional Analysis \u2014 Analysis functions are stateless transformations</li> </ol>"},{"location":"API_CONTRACT/#2-core-data-structures","title":"2. Core Data Structures","text":""},{"location":"API_CONTRACT/#21-attentioncapture","title":"2.1 AttentionCapture","text":"<p>The primary output of the capture system. Immutable container for captured attention data.</p> <pre><code>@dataclass(frozen=True)\nclass AttentionCapture:\n    \"\"\"\n    Immutable container for captured attention data from a forward pass.\n\n    All tensors are detached from computation graph and on CPU.\n    \"\"\"\n\n    # Query vectors: [n_layers, n_heads, n_tokens, head_dim]\n    queries: torch.Tensor\n\n    # Key vectors: [n_layers, n_kv_heads, n_tokens, head_dim]\n    keys: torch.Tensor\n\n    # Value vectors: [n_layers, n_kv_heads, n_tokens, head_dim]\n    values: torch.Tensor\n\n    # Attention patterns: [n_layers, n_heads, n_tokens, n_tokens]\n    # patterns[l, h, i, j] = attention from token i to token j at layer l, head h\n    patterns: torch.Tensor\n\n    # Metadata\n    n_layers: int\n    n_heads: int\n    n_kv_heads: int\n    n_tokens: int\n    head_dim: int\n    model_name: str\n\n    @property\n    def is_gqa(self) -&gt; bool:\n        \"\"\"True if model uses Grouped Query Attention.\"\"\"\n        return self.n_heads != self.n_kv_heads\n\n    @property\n    def gqa_ratio(self) -&gt; int:\n        \"\"\"Number of query heads per KV head.\"\"\"\n        return self.n_heads // self.n_kv_heads\n</code></pre> <p>Shape Contracts:</p> Tensor Shape Dtype Description <code>queries</code> <code>[L, H_q, T, D]</code> float32 Query vectors <code>keys</code> <code>[L, H_kv, T, D]</code> float32 Key vectors <code>values</code> <code>[L, H_kv, T, D]</code> float32 Value vectors <code>patterns</code> <code>[L, H_q, T, T]</code> float32 Attention probabilities <p>Where:</p> <ul> <li><code>L</code> = number of layers</li> <li><code>H_q</code> = number of query heads</li> <li><code>H_kv</code> = number of key/value heads (may differ from H_q in GQA)</li> <li><code>T</code> = number of tokens (sequence length)</li> <li><code>D</code> = head dimension</li> </ul> <p>Invariants:</p> <ul> <li>All tensors are on CPU</li> <li>All tensors are detached (no gradients)</li> <li><code>patterns</code> sums to 1.0 along last dimension (softmax output)</li> <li><code>patterns[l, h, i, j] = 0</code> for <code>j &gt; i</code> (causal masking)</li> </ul>"},{"location":"API_CONTRACT/#22-attentionstructure","title":"2.2 AttentionStructure","text":"<p>Semantic description of a model's attention architecture.</p> <pre><code>@dataclass(frozen=True)\nclass AttentionStructure:\n    \"\"\"\n    Describes the attention structure at a given layer.\n\n    Abstracts away model-specific details into semantic categories.\n    \"\"\"\n    n_query_heads: int\n    n_kv_heads: int\n    head_dim: int\n\n    @property\n    def attention_type(self) -&gt; str:\n        \"\"\"Returns 'MHA', 'GQA', or 'MQA'.\"\"\"\n        if self.n_query_heads == self.n_kv_heads:\n            return \"MHA\"  # Multi-Head Attention\n        elif self.n_kv_heads == 1:\n            return \"MQA\"  # Multi-Query Attention\n        else:\n            return \"GQA\"  # Grouped Query Attention\n\n    @property\n    def gqa_ratio(self) -&gt; int:\n        \"\"\"Query heads per KV head.\"\"\"\n        return self.n_query_heads // self.n_kv_heads\n</code></pre>"},{"location":"API_CONTRACT/#23-captureconfig","title":"2.3 CaptureConfig","text":"<p>Configuration for what to capture during a forward pass.</p> <pre><code>@dataclass\nclass CaptureConfig:\n    \"\"\"\n    Specifies what attention data to capture.\n\n    Use to control memory usage by capturing only needed components.\n    \"\"\"\n\n    # Which layers to capture (None = all)\n    layers: Optional[List[int]] = None\n\n    # Which components to capture\n    capture_queries: bool = True\n    capture_keys: bool = True\n    capture_values: bool = True\n    capture_patterns: bool = True\n\n    # Capture stage for Q/K vectors\n    # 'pre_rope': Before rotary position embedding (position-agnostic)\n    # 'post_rope': After rotary position embedding (position-aware)\n    qk_stage: Literal['pre_rope', 'post_rope'] = 'pre_rope'\n\n    def validate(self, model_adapter: 'ModelAdapter') -&gt; None:\n        \"\"\"\n        Validate config against model.\n\n        Raises:\n            ValueError: If layers out of range or invalid combination\n        \"\"\"\n        ...\n</code></pre> <p>Validation Rules:</p> <ul> <li><code>layers</code> must be in range <code>[0, model.num_layers())</code></li> <li>At least one capture flag must be True</li> <li>Raises <code>ValueError</code> with descriptive message on failure</li> </ul>"},{"location":"API_CONTRACT/#3-model-adapter-interface","title":"3. Model Adapter Interface","text":""},{"location":"API_CONTRACT/#31-modeladapter-abstract","title":"3.1 ModelAdapter (Abstract)","text":"<p>The public interface for model interaction. Delegates to private implementations.</p> <pre><code>class ModelAdapter(ABC):\n    \"\"\"\n    Model-agnostic interface for transformer instrumentation.\n\n    Public methods define WHAT operations are available.\n    Private adapters implement HOW for specific model families.\n    \"\"\"\n\n    @abstractmethod\n    def num_layers(self) -&gt; int:\n        \"\"\"Total number of transformer layers.\"\"\"\n        ...\n\n    @abstractmethod\n    def num_heads(self, layer: int = 0) -&gt; int:\n        \"\"\"Number of query attention heads at given layer.\"\"\"\n        ...\n\n    @abstractmethod\n    def attention_structure(self, layer: int = 0) -&gt; AttentionStructure:\n        \"\"\"\n        Returns semantic description of attention at given layer.\n\n        Use this to determine GQA/MQA structure without model-specific logic.\n        \"\"\"\n        ...\n\n    @abstractmethod\n    def capture(\n        self,\n        input_ids: torch.Tensor,\n        config: CaptureConfig = None\n    ) -&gt; AttentionCapture:\n        \"\"\"\n        Run forward pass and capture attention data.\n\n        Args:\n            input_ids: Token IDs [batch=1, seq_len]\n            config: What to capture (default: all components, all layers)\n\n        Returns:\n            AttentionCapture with requested data\n\n        Raises:\n            ValueError: If config validation fails\n            RuntimeError: If capture hooks fail\n        \"\"\"\n        ...\n\n    @abstractmethod\n    def generate(\n        self,\n        prompt: str,\n        max_new_tokens: int = 100,\n        temperature: float = 1.0,\n        **kwargs\n    ) -&gt; str:\n        \"\"\"Generate text completion.\"\"\"\n        ...\n</code></pre> <p>Guarantees:</p> <ul> <li><code>capture()</code> is deterministic given same inputs</li> <li><code>capture()</code> does not modify model weights</li> <li>All returned tensors are detached and on CPU</li> </ul>"},{"location":"API_CONTRACT/#4-analysis-layer","title":"4. Analysis Layer","text":""},{"location":"API_CONTRACT/#41-design-contract","title":"4.1 Design Contract","text":"<p>All analysis functions follow this pattern:</p> <pre><code>def analysis_function(capture: AttentionCapture, **params) -&gt; torch.Tensor:\n    \"\"\"\n    Pure function: AttentionCapture \u2192 Tensor\n\n    - No side effects\n    - No model access\n    - No plotting\n    - Deterministic output\n    \"\"\"\n</code></pre>"},{"location":"API_CONTRACT/#42-normanalysis","title":"4.2 NormAnalysis","text":"<p>Compute vector norms from captured data.</p> <pre><code>class NormAnalysis:\n    \"\"\"Query, Key, Value norm computations.\"\"\"\n\n    @staticmethod\n    def q_norms(capture: AttentionCapture) -&gt; torch.Tensor:\n        \"\"\"\n        L2 norms of query vectors.\n\n        Args:\n            capture: AttentionCapture from model\n\n        Returns:\n            Tensor of shape [n_layers, n_heads, n_tokens]\n\n        Formula:\n            ||Q||\u2082 = \u221a(\u03a3\u1d62 Q\u1d62\u00b2)\n        \"\"\"\n        ...\n\n    @staticmethod\n    def k_norms(capture: AttentionCapture) -&gt; torch.Tensor:\n        \"\"\"\n        L2 norms of key vectors.\n\n        Returns:\n            Tensor of shape [n_layers, n_kv_heads, n_tokens]\n        \"\"\"\n        ...\n\n    @staticmethod\n    def v_norms(capture: AttentionCapture) -&gt; torch.Tensor:\n        \"\"\"\n        L2 norms of value vectors.\n\n        Returns:\n            Tensor of shape [n_layers, n_kv_heads, n_tokens]\n        \"\"\"\n        ...\n</code></pre> <p>Shape Contracts:</p> Method Output Shape Description <code>q_norms()</code> <code>[L, H_q, T]</code> Query norms per layer/head/token <code>k_norms()</code> <code>[L, H_kv, T]</code> Key norms per layer/head/token <code>v_norms()</code> <code>[L, H_kv, T]</code> Value norms per layer/head/token"},{"location":"API_CONTRACT/#43-entropyanalysis","title":"4.3 EntropyAnalysis","text":"<p>Compute entropy metrics from attention patterns.</p> <pre><code>class EntropyAnalysis:\n    \"\"\"Attention entropy computations.\"\"\"\n\n    @staticmethod\n    def token_entropy(\n        capture: AttentionCapture,\n        ignore_first: int = 2\n    ) -&gt; torch.Tensor:\n        \"\"\"\n        Shannon entropy of attention distribution per token.\n\n        Args:\n            capture: AttentionCapture from model\n            ignore_first: Set first N tokens to NaN (insufficient context)\n\n        Returns:\n            Tensor of shape [n_layers, n_heads, n_tokens]\n\n        Formula:\n            H(t) = -\u03a3\u2c7c\u208c\u2080\u1d57 p(j|t) \u00b7 log(p(j|t))\n\n            Where p(j|t) = attention weight from token t to token j\n            Only sums over j \u2264 t (causal masking)\n\n        Interpretation:\n            - High entropy \u2192 diffuse attention (attending to many tokens)\n            - Low entropy \u2192 focused attention (attending to few tokens)\n            - H = 0 \u2192 deterministic attention (100% on one token)\n            - H = log(t) \u2192 uniform attention over t tokens\n        \"\"\"\n        ...\n\n    @staticmethod\n    def effective_rank(capture: AttentionCapture) -&gt; torch.Tensor:\n        \"\"\"\n        Effective number of attended tokens.\n\n        Returns:\n            Tensor of shape [n_layers, n_heads, n_tokens]\n\n        Formula:\n            eff_rank = exp(H)\n\n            Where H is Shannon entropy.\n\n        Interpretation:\n            If eff_rank = 3.5, attention is spread as if uniformly\n            attending to ~3.5 tokens.\n        \"\"\"\n        ...\n\n    @staticmethod\n    def delta_entropy(\n        capture_treatment: AttentionCapture,\n        capture_control: AttentionCapture,\n        method: Literal['mean', 'final'] = 'mean'\n    ) -&gt; torch.Tensor:\n        \"\"\"\n        Difference in entropy between two conditions.\n\n        Args:\n            capture_treatment: Capture under treatment condition\n            capture_control: Capture under control condition\n            method:\n                'mean': Compare mean entropy across positions\n                'final': Compare entropy at final token only\n\n        Returns:\n            Tensor of shape [n_layers, n_heads]\n\n        Formula:\n            \u0394H = mean(H_treatment) - mean(H_control)\n\n        Interpretation:\n            - \u0394H &gt; 0 \u2192 Treatment causes more diffuse attention\n            - \u0394H &lt; 0 \u2192 Treatment causes more focused attention\n        \"\"\"\n        ...\n</code></pre> <p>Shape Contracts:</p> Method Output Shape Description <code>token_entropy()</code> <code>[L, H_q, T]</code> Entropy per position <code>effective_rank()</code> <code>[L, H_q, T]</code> Effective attended tokens <code>delta_entropy()</code> <code>[L, H_q]</code> Delta between conditions"},{"location":"API_CONTRACT/#44-attentionanalysis","title":"4.4 AttentionAnalysis","text":"<p>Compute attention pattern metrics.</p> <pre><code>class AttentionAnalysis:\n    \"\"\"Attention pattern analysis.\"\"\"\n\n    @staticmethod\n    def max_weight(capture: AttentionCapture) -&gt; torch.Tensor:\n        \"\"\"\n        Maximum attention weight per token.\n\n        Returns:\n            Tensor of shape [n_layers, n_heads, n_tokens]\n\n        Formula:\n            max_attn(t) = max\u2c7c\u2264\u209c p(j|t)\n        \"\"\"\n        ...\n\n    @staticmethod\n    def effective_span(\n        capture: AttentionCapture,\n        threshold: float = 0.9\n    ) -&gt; torch.Tensor:\n        \"\"\"\n        Minimum tokens needed to capture threshold of attention mass.\n\n        Args:\n            capture: AttentionCapture from model\n            threshold: Cumulative attention threshold (default 0.9 = 90%)\n\n        Returns:\n            Tensor of shape [n_layers, n_heads, n_tokens]\n            Values are integers (as float32 for consistency)\n\n        Formula:\n            k_eff(t) = argmin_k { \u03a3\u1d62\u208c\u2081\u1d4f sorted_weights[i] \u2265 threshold }\n\n            Where sorted_weights are attention weights sorted descending.\n\n        Interpretation:\n            If k_eff = 5, the top 5 tokens account for \u226590% of attention.\n        \"\"\"\n        ...\n\n    @staticmethod\n    def attention_to_position(\n        capture: AttentionCapture,\n        target_positions: List[int]\n    ) -&gt; torch.Tensor:\n        \"\"\"\n        Attention weight to specific token positions.\n\n        Args:\n            capture: AttentionCapture from model\n            target_positions: List of position indices to measure\n\n        Returns:\n            Tensor of shape [n_layers, n_heads, n_tokens, len(target_positions)]\n        \"\"\"\n        ...\n</code></pre> <p>Shape Contracts:</p> Method Output Shape Description <code>max_weight()</code> <code>[L, H_q, T]</code> Peak attention per token <code>effective_span()</code> <code>[L, H_q, T]</code> k_eff metric <code>attention_to_position()</code> <code>[L, H_q, T, P]</code> Attention to P target positions"},{"location":"API_CONTRACT/#45-correlationanalysis","title":"4.5 CorrelationAnalysis","text":"<p>Statistical correlation between metrics.</p> <pre><code>class CorrelationAnalysis:\n    \"\"\"Cross-metric correlation analysis.\"\"\"\n\n    @staticmethod\n    def pearson(\n        x: torch.Tensor,\n        y: torch.Tensor,\n        dim: int = -1\n    ) -&gt; torch.Tensor:\n        \"\"\"\n        Pearson correlation coefficient.\n\n        Args:\n            x, y: Tensors of same shape\n            dim: Dimension to correlate along\n\n        Returns:\n            Correlation tensor with dim reduced\n\n        Formula:\n            r = \u03a3(x - \u03bc\u2093)(y - \u03bc\u1d67) / (\u03c3\u2093 \u00b7 \u03c3\u1d67 \u00b7 n)\n        \"\"\"\n        ...\n\n    @staticmethod\n    def spearman(x: torch.Tensor, y: torch.Tensor, dim: int = -1) -&gt; torch.Tensor:\n        \"\"\"Spearman rank correlation.\"\"\"\n        ...\n\n    @staticmethod\n    def norm_entropy_correlation(capture: AttentionCapture) -&gt; torch.Tensor:\n        \"\"\"\n        Correlation between query norm and attention entropy.\n\n        Returns:\n            Tensor of shape [n_layers, n_heads]\n\n        This is the core metric for the Oculi hypothesis:\n        Strong negative correlation suggests Q magnitude controls attention focus.\n        \"\"\"\n        ...\n</code></pre>"},{"location":"API_CONTRACT/#5-intervention-layer","title":"5. Intervention Layer","text":""},{"location":"API_CONTRACT/#51-intervention-semantics","title":"5.1 Intervention Semantics","text":"<p>CRITICAL DEFINITION:</p> <p>When we \"scale Q by \u03b1\", we mean:</p> <pre><code>Q_scaled = \u03b1 \u00b7 Q\n</code></pre> <p>This is raw magnitude scaling, NOT direction-preserving normalization.</p> <p>Mathematical Consequence:</p> <p>Scaling Q by \u03b1 affects attention logits as:</p> <pre><code>logits_scaled = (\u03b1Q) \u00b7 K\u1d40 / \u221ad = \u03b1 \u00b7 (Q \u00b7 K\u1d40 / \u221ad) = \u03b1 \u00b7 logits_original\n</code></pre> <p>After softmax, this sharpens (\u03b1 &gt; 1) or flattens (\u03b1 &lt; 1) attention.</p> <p>The Oculi Method scales both Q and K by \u221a\u03b1 to achieve equivalent effect:</p> <pre><code>logits = (\u221a\u03b1 \u00b7 Q) \u00b7 (\u221a\u03b1 \u00b7 K)\u1d40 / \u221ad = \u03b1 \u00b7 logits_original\n</code></pre>"},{"location":"API_CONTRACT/#52-qscaler","title":"5.2 QScaler","text":"<pre><code>@dataclass\nclass QScaler:\n    \"\"\"\n    Intervention that scales query vectors at a specific head.\n\n    Semantic Definition:\n        Q_new = alpha \u00b7 Q_original\n\n    Effect on Attention:\n        alpha &gt; 1.0 \u2192 Sharpen attention (increase focus)\n        alpha &lt; 1.0 \u2192 Flatten attention (decrease focus)\n        alpha = 1.0 \u2192 No change (identity)\n\n    This is RAW magnitude scaling, not direction-preserving.\n    The attention logits scale linearly with alpha.\n    \"\"\"\n    layer: int\n    head: int\n    alpha: float\n\n    def validate(self, adapter: ModelAdapter) -&gt; None:\n        \"\"\"\n        Validate intervention parameters.\n\n        Raises:\n            ValueError: If layer or head out of range\n            ValueError: If alpha &lt;= 0\n        \"\"\"\n        if not 0 &lt;= self.layer &lt; adapter.num_layers():\n            raise ValueError(f\"Layer {self.layer} out of range [0, {adapter.num_layers()})\")\n        if not 0 &lt;= self.head &lt; adapter.num_heads(self.layer):\n            raise ValueError(f\"Head {self.head} out of range [0, {adapter.num_heads(self.layer)})\")\n        if self.alpha &lt;= 0:\n            raise ValueError(f\"Alpha must be positive, got {self.alpha}\")\n</code></pre>"},{"location":"API_CONTRACT/#53-kscaler","title":"5.3 KScaler","text":"<pre><code>@dataclass\nclass KScaler:\n    \"\"\"\n    Intervention that scales key vectors at a specific KV-head.\n\n    For GQA models: Affects all query heads sharing this KV-head.\n\n    Semantic Definition:\n        K_new = alpha \u00b7 K_original\n    \"\"\"\n    layer: int\n    kv_head: int  # Note: KV-head index, not query head\n    alpha: float\n</code></pre>"},{"location":"API_CONTRACT/#54-oculiscaler","title":"5.4 OculiScaler","text":"<pre><code>@dataclass\nclass OculiScaler:\n    \"\"\"\n    The Oculi intervention: scale both Q and K by \u221a\u03b1.\n\n    Semantic Definition:\n        Q_new = \u221a\u03b1 \u00b7 Q_original\n        K_new = \u221a\u03b1 \u00b7 K_original\n\n    Net Effect:\n        logits_new = \u03b1 \u00b7 logits_original\n\n    This achieves the same attention sharpening as scaling logits by \u03b1,\n    but intervenes at the representation level rather than post-computation.\n\n    Mathematical Justification:\n        softmax(\u03b1 \u00b7 QK\u1d40/\u221ad) \u2248 softmax((\u221a\u03b1\u00b7Q)(\u221a\u03b1\u00b7K)\u1d40/\u221ad)\n    \"\"\"\n    layer: int\n    head: int\n    alpha: float  # Net scaling factor (internal uses \u221a\u03b1 on each)\n</code></pre>"},{"location":"API_CONTRACT/#55-headablation","title":"5.5 HeadAblation","text":"<pre><code>@dataclass\nclass HeadAblation:\n    \"\"\"\n    Intervention that zeros out a head's output.\n\n    Semantic Definition:\n        head_output_new = 0\n\n    Use for: Measuring causal importance of specific heads.\n    \"\"\"\n    layer: int\n    head: int\n</code></pre>"},{"location":"API_CONTRACT/#56-interventioncontext","title":"5.6 InterventionContext","text":"<pre><code>class InterventionContext:\n    \"\"\"\n    Context manager for applying interventions during generation.\n\n    Usage:\n        scaler = OculiScaler(layer=23, head=5, alpha=1.5)\n\n        with InterventionContext(adapter, [scaler]):\n            output = adapter.generate(prompt)\n\n    Interventions are automatically removed on context exit.\n    \"\"\"\n\n    def __init__(\n        self,\n        adapter: ModelAdapter,\n        interventions: List[Union[QScaler, KScaler, OculiScaler, HeadAblation]]\n    ):\n        ...\n\n    def __enter__(self) -&gt; 'InterventionContext':\n        \"\"\"Apply all interventions.\"\"\"\n        ...\n\n    def __exit__(self, *args) -&gt; None:\n        \"\"\"Remove all interventions.\"\"\"\n        ...\n</code></pre>"},{"location":"API_CONTRACT/#6-visualization-layer","title":"6. Visualization Layer","text":""},{"location":"API_CONTRACT/#61-design-contract","title":"6.1 Design Contract","text":"<p>Visualization functions return <code>matplotlib.Figure</code> objects. User controls saving.</p> <pre><code>def plot_function(data: torch.Tensor, **params) -&gt; matplotlib.Figure:\n    \"\"\"\n    Pure function: Tensor \u2192 Figure\n\n    - No side effects (no plt.show(), no saving)\n    - Returns Figure for user to display/save\n    - Uses consistent Oculi color scheme\n    \"\"\"\n</code></pre>"},{"location":"API_CONTRACT/#62-available-plots","title":"6.2 Available Plots","text":"<pre><code>class EntropyPlots:\n    \"\"\"Entropy visualization.\"\"\"\n\n    @staticmethod\n    def heatmap(\n        entropy: torch.Tensor,\n        title: str = \"Attention Entropy\"\n    ) -&gt; Figure:\n        \"\"\"\n        Entropy heatmap across layers and heads.\n\n        Args:\n            entropy: Shape [n_layers, n_heads] (mean across positions)\n\n        Returns:\n            Figure with heatmap, layer on Y-axis, head on X-axis\n        \"\"\"\n        ...\n\n    @staticmethod\n    def distribution(\n        entropy: torch.Tensor,\n        title: str = \"Entropy Distribution\"\n    ) -&gt; Figure:\n        \"\"\"\n        Histogram of entropy values across all positions.\n        \"\"\"\n        ...\n\n\nclass InterventionPlots:\n    \"\"\"Intervention effect visualization.\"\"\"\n\n    @staticmethod\n    def alpha_curve(\n        alphas: List[float],\n        metric_values: List[float],\n        metric_name: str = \"Accuracy\",\n        title: str = \"Intervention Effect\"\n    ) -&gt; Figure:\n        \"\"\"\n        Plot metric vs alpha (the \"Goldilocks curve\").\n\n        Annotates peak value and location.\n        \"\"\"\n        ...\n\n    @staticmethod\n    def delta_entropy_heatmap(\n        delta_entropy: torch.Tensor,\n        title: str = \"\u0394Entropy (Treatment - Control)\"\n    ) -&gt; Figure:\n        \"\"\"\n        Heatmap of entropy change per layer/head.\n\n        Diverging colormap: red = increase, blue = decrease.\n        \"\"\"\n        ...\n\n\nclass CorrelationPlots:\n    \"\"\"Correlation visualization.\"\"\"\n\n    @staticmethod\n    def scatter(\n        x: torch.Tensor,\n        y: torch.Tensor,\n        x_label: str,\n        y_label: str,\n        title: str = \"Correlation\"\n    ) -&gt; Figure:\n        \"\"\"\n        Scatter plot with regression line and correlation coefficient.\n        \"\"\"\n        ...\n</code></pre>"},{"location":"API_CONTRACT/#7-failure-modes","title":"7. Failure Modes","text":""},{"location":"API_CONTRACT/#71-expected-errors","title":"7.1 Expected Errors","text":"Condition Exception Message Layer out of range <code>ValueError</code> \"Layer {n} out of range [0, {max})\" Head out of range <code>ValueError</code> \"Head {n} out of range [0, {max})\" Alpha \u2264 0 <code>ValueError</code> \"Alpha must be positive, got {alpha}\" Invalid QK stage <code>ValueError</code> \"qk_stage must be 'pre_rope' or 'post_rope'\" Unsupported model <code>UnsupportedModelError</code> \"No adapter for model: {name}\" Hook failure <code>RuntimeError</code> \"Capture failed at layer {n}: {details}\""},{"location":"API_CONTRACT/#72-edge-cases","title":"7.2 Edge Cases","text":"Input Behavior Sequence length = 1 <code>entropy[0] = NaN</code> (no prior context) Empty layers list Capture all layers <code>ignore_first &gt; seq_len</code> All entropy values = NaN GQA head mapping K/V operations use KV-head index"},{"location":"API_CONTRACT/#8-versioning-policy","title":"8. Versioning Policy","text":""},{"location":"API_CONTRACT/#81-semantic-versioning","title":"8.1 Semantic Versioning","text":"<p>Format: <code>MAJOR.MINOR.PATCH</code></p> <p>MAJOR (1.0 \u2192 2.0):</p> <ul> <li>Changing tensor output shapes</li> <li>Renaming public classes/methods</li> <li>Changing mathematical definitions</li> <li>Breaking <code>AttentionCapture</code> structure</li> </ul> <p>MINOR (1.0 \u2192 1.1):</p> <ul> <li>Adding new analysis methods</li> <li>Adding new model adapters</li> <li>New optional parameters with defaults</li> <li>New visualization functions</li> </ul> <p>PATCH (1.0.0 \u2192 1.0.1):</p> <ul> <li>Bug fixes in private layer</li> <li>Performance improvements</li> <li>Documentation fixes</li> <li>Test additions</li> </ul>"},{"location":"API_CONTRACT/#82-pre-10-warning","title":"8.2 Pre-1.0 Warning","text":"<p>[!CAUTION] Versions <code>0.x.y</code> are pre-release. Public API may change between minor versions. Production use not recommended until <code>1.0.0</code>.</p>"},{"location":"API_CONTRACT/#9-private-layer-scope","title":"9. Private Layer Scope","text":""},{"location":"API_CONTRACT/#91-directory-structure","title":"9.1 Directory Structure","text":"<pre><code>oculi/_private/\n\u251c\u2500\u2500 adapters/           # Model-specific implementations\n\u2502   \u251c\u2500\u2500 llama.py        # LlamaForCausalLM adapter\n\u2502   \u251c\u2500\u2500 mistral.py      # Mistral adapter\n\u2502   \u251c\u2500\u2500 qwen.py         # Qwen2/2.5 adapter\n\u2502   \u2514\u2500\u2500 falcon.py       # Falcon adapter\n\u2502\n\u251c\u2500\u2500 hooks/              # PyTorch hook machinery\n\u2502   \u251c\u2500\u2500 capture.py      # Forward hook registration\n\u2502   \u251c\u2500\u2500 intervention.py # Intervention hook implementation\n\u2502   \u2514\u2500\u2500 utils.py        # Hook utilities\n\u2502\n\u251c\u2500\u2500 cache/              # Memory optimization\n\u2502   \u2514\u2500\u2500 activation.py   # Activation caching\n\u2502\n\u2514\u2500\u2500 validation/         # Sanity checks\n    \u2514\u2500\u2500 checks.py       # Internal validation\n</code></pre>"},{"location":"API_CONTRACT/#92-private-layer-rules","title":"9.2 Private Layer Rules","text":"<ol> <li>PRs Accepted For: New model adapters, bug fixes, performance improvements</li> <li>PRs NOT Accepted For: Changes that affect public API semantics</li> <li>Internal Refactoring: Allowed freely without version bumps</li> <li>No Public Imports: Nothing from <code>_private/</code> appears in public <code>__init__.py</code></li> </ol>"},{"location":"API_CONTRACT/#10-explicit-non-guarantees","title":"10. Explicit Non-Guarantees","text":"<p>We do NOT guarantee:</p> <ol> <li>Performance \u2014 Capture may be memory-intensive; optimize in private layer</li> <li>Internal Tensor Layouts \u2014 Private tensors may have different shapes</li> <li>Hook Ordering \u2014 Order of hook execution is implementation detail</li> <li>GPU Memory \u2014 Large sequences may require chunking (user responsibility)</li> <li>Thread Safety \u2014 Single-threaded capture assumed</li> <li>Gradient Flow \u2014 All captures are inference-only (no backprop)</li> </ol>"},{"location":"API_CONTRACT/#11-contract-test-examples","title":"11. Contract Test Examples","text":"<p>These tests define the contract independent of implementation.</p> <pre><code># tests/contract_tests/test_shapes.py\n\ndef test_q_norms_shape():\n    \"\"\"Verify q_norms returns documented shape.\"\"\"\n    capture = mock_capture(n_layers=4, n_heads=8, n_tokens=16, head_dim=64)\n    norms = NormAnalysis.q_norms(capture)\n    assert norms.shape == (4, 8, 16)  # [L, H, T]\n\n\ndef test_token_entropy_shape():\n    \"\"\"Verify entropy returns documented shape.\"\"\"\n    capture = mock_capture(n_layers=4, n_heads=8, n_tokens=16, head_dim=64)\n    entropy = EntropyAnalysis.token_entropy(capture)\n    assert entropy.shape == (4, 8, 16)  # [L, H, T]\n\n\ndef test_entropy_causal_masking():\n    \"\"\"Verify entropy respects causal structure.\"\"\"\n    # Token 0 attends only to itself \u2192 zero entropy\n    # Token 1 attends uniformly to 0,1 \u2192 H = log(2)\n    patterns = torch.tensor([[\n        [[1.0, 0.0, 0.0],\n         [0.5, 0.5, 0.0],\n         [0.33, 0.33, 0.34]]\n    ]])  # [1, 1, 3, 3]\n\n    capture = AttentionCapture(\n        patterns=patterns,\n        # ... other fields\n    )\n\n    entropy = EntropyAnalysis.token_entropy(capture, ignore_first=0)\n\n    assert torch.isclose(entropy[0, 0, 0], torch.tensor(0.0), atol=1e-3)\n    assert torch.isclose(entropy[0, 0, 1], torch.tensor(0.693), atol=1e-2)  # log(2)\n\n\ndef test_oculi_scaler_symmetry():\n    \"\"\"Verify OculiScaler applies \u221a\u03b1 to both Q and K.\"\"\"\n    # This is a semantic test: we verify the EFFECT, not implementation\n    # With \u03b1=4, Q and K should each be scaled by 2\n    # Net effect on logits = 4x\n    pass  # Implementation in integration tests\n</code></pre>"},{"location":"API_CONTRACT/#appendix-a-mathematical-reference","title":"Appendix A: Mathematical Reference","text":""},{"location":"API_CONTRACT/#shannon-entropy","title":"Shannon Entropy","text":"<pre><code>H(t) = -\u03a3\u2c7c\u208c\u2080\u1d57 p(j|t) \u00b7 log(p(j|t))\n\nWhere:\n- t = query token position\n- j = key token position (j \u2264 t due to causal masking)\n- p(j|t) = attention probability from t to j\n</code></pre>"},{"location":"API_CONTRACT/#effective-rank","title":"Effective Rank","text":"<pre><code>eff_rank = exp(H)\n\nInterpretation: Number of tokens if attention were uniform\n</code></pre>"},{"location":"API_CONTRACT/#effective-span-k_eff","title":"Effective Span (k_eff)","text":"<pre><code>k_eff(t) = min{k : \u03a3\u1d62\u208c\u2081\u1d4f sorted_p[i] \u2265 0.9}\n\nWhere sorted_p is attention weights sorted in descending order\n</code></pre>"},{"location":"API_CONTRACT/#l2-norm","title":"L2 Norm","text":"<pre><code>||Q||\u2082 = \u221a(\u03a3\u1d62 Q\u1d62\u00b2)\n</code></pre>"},{"location":"API_CONTRACT/#pearson-correlation","title":"Pearson Correlation","text":"<pre><code>r = \u03a3\u1d62(x\u1d62 - \u03bc\u2093)(y\u1d62 - \u03bc\u1d67) / (n \u00b7 \u03c3\u2093 \u00b7 \u03c3\u1d67)\n</code></pre>"},{"location":"API_CONTRACT/#appendix-b-supported-models","title":"Appendix B: Supported Models","text":"Model Family Adapter Attention Type Notes LLaMA 2/3 <code>LlamaAdapter</code> GQA (4:1 or 8:1) Primary support Mistral <code>MistralAdapter</code> GQA (4:1) Sliding window Qwen 2/2.5 <code>QwenAdapter</code> GQA (7:1) Via TransformerLens Falcon <code>FalconAdapter</code> MQA (71:1) Multi-query <p>End of API Contract Specification</p>"},{"location":"getting-started/core-concepts/","title":"Core Concepts","text":"<p>Understanding Oculi's design philosophy and key abstractions.</p>"},{"location":"getting-started/core-concepts/#design-philosophy","title":"Design Philosophy","text":"<p>Oculi is built on four core principles:</p>"},{"location":"getting-started/core-concepts/#1-learning-first-design","title":"1. Learning-First Design","text":"<p>Adapters are executable documentation.</p> <p>Instead of hiding model internals behind abstractions, Oculi makes them explicit. Each adapter is a learning resource that shows you exactly where every component lives in the model.</p> <pre><code># Traditional approach (hidden)\ncapture = magic_capture(model, input)  # How does this work? \ud83e\udd37\n\n# Oculi approach (explicit)\nfrom oculi.models.llama import LlamaAttentionAdapter\nadapter = LlamaAttentionAdapter(model, tokenizer)\n# See exactly what's happening in adapter.py\n</code></pre> <p>See: <code>oculi/models/llama/anatomy.py</code> - Full documentation of model structure</p>"},{"location":"getting-started/core-concepts/#2-explicit-control","title":"2. Explicit Control","text":"<p>No magic. You choose what to capture.</p> <p>Oculi never auto-detects or makes assumptions. You explicitly specify what you want:</p> <pre><code>from oculi import CaptureConfig\n\n# Explicit configuration\nconfig = CaptureConfig(\n    layers=[20, 21, 22],       # Exactly these layers\n    capture_queries=True,       # Yes to queries\n    capture_values=False,       # No to values\n    qk_stage='post_rope'        # After RoPE\n)\n\ncapture = adapter.capture(input_ids, config=config)\n</code></pre>"},{"location":"getting-started/core-concepts/#3-pure-functional-analysis","title":"3. Pure Functional Analysis","text":"<p>Stateless, deterministic, testable.</p> <p>All analysis functions are pure:</p> <pre><code>def analysis_function(capture: AttentionCapture, **params) -&gt; torch.Tensor:\n    \"\"\"\n    Pure function: AttentionCapture \u2192 Tensor\n\n    - No side effects\n    - No model access\n    - No plotting\n    - Deterministic output\n    \"\"\"\n</code></pre> <p>Benefits: - Same inputs \u2192 Same outputs - Easy to test - Easy to understand - Easy to compose</p>"},{"location":"getting-started/core-concepts/#4-memory-conscious","title":"4. Memory-Conscious","text":"<p>Selective capture, efficient storage.</p> <p>Transformers are memory-intensive. Oculi gives you control:</p> <pre><code># Memory strategies\nconfig1 = CaptureConfig(layers=[20, 21, 22])  # Only specific layers\nconfig2 = CaptureConfig(capture_values=False)  # Skip values\nconfig3 = LogitConfig(top_k=10)  # Top-k only for logits\n\n# Choose what you need\ncapture = adapter.capture(input_ids, config1)\n</code></pre>"},{"location":"getting-started/core-concepts/#key-abstractions","title":"Key Abstractions","text":""},{"location":"getting-started/core-concepts/#attentioncapture","title":"AttentionCapture","text":"<p>The core data structure for captured attention:</p> <pre><code>@dataclass(frozen=True)\nclass AttentionCapture:\n    queries: torch.Tensor   # [L, H, T, D]\n    keys: torch.Tensor      # [L, H_kv, T, D]\n    values: torch.Tensor    # [L, H_kv, T, D]\n    patterns: torch.Tensor  # [L, H, T, T]\n\n    n_layers: int\n    n_heads: int\n    n_kv_heads: int\n    n_tokens: int\n    head_dim: int\n</code></pre> <p>Invariants: - All tensors on CPU, detached - <code>patterns[l, h, i, j]</code> = attention from token i to token j - Patterns sum to 1.0 (softmax) - Causal masking enforced</p>"},{"location":"getting-started/core-concepts/#residualcapture","title":"ResidualCapture","text":"<p>Residual stream at four intervention points:</p> <pre><code>@dataclass(frozen=True)\nclass ResidualCapture:\n    pre_attn: Optional[torch.Tensor]   # [L, T, H] - Before attention\n    post_attn: Optional[torch.Tensor]  # [L, T, H] - After attention\n    pre_mlp: Optional[torch.Tensor]    # [L, T, H] - Before MLP\n    post_mlp: Optional[torch.Tensor]   # [L, T, H] - After MLP\n</code></pre> <p>Usage: Understanding information flow through residual stream</p>"},{"location":"getting-started/core-concepts/#model-adapters","title":"Model Adapters","text":"<p>Bridge between Oculi's API and specific model architectures:</p> <pre><code>class ModelAdapter(ABC):\n    @abstractmethod\n    def capture(self, input_ids, config) -&gt; AttentionCapture:\n        \"\"\"Run forward pass and capture attention.\"\"\"\n\n    @abstractmethod\n    def attention_structure(self, layer) -&gt; AttentionStructure:\n        \"\"\"Describe attention architecture.\"\"\"\n</code></pre> <p>Current Adapters: - <code>LlamaAttentionAdapter</code> - LLaMA 2/3 (GQA support) - More coming soon (Mistral, Qwen)</p>"},{"location":"getting-started/core-concepts/#analysis-workflow","title":"Analysis Workflow","text":""},{"location":"getting-started/core-concepts/#1-capture","title":"1. Capture","text":"<pre><code># Basic capture\ncapture = adapter.capture(input_ids)\n\n# Full capture (everything)\nfull = adapter.capture_full(input_ids)\n</code></pre>"},{"location":"getting-started/core-concepts/#2-analyze","title":"2. Analyze","text":"<pre><code>from oculi.analysis import EntropyAnalysis, AttributionMethods\n\n# Compute metrics\nentropy = EntropyAnalysis.token_entropy(capture)\n\n# Attribution\nattribution = AttributionMethods.direct_logit_attribution(\n    full.residual, unembed, target_token\n)\n</code></pre>"},{"location":"getting-started/core-concepts/#3-interpret","title":"3. Interpret","text":"<pre><code># Find important layers\nimportant_layers = attribution.values.abs().topk(5)\n\n# Examine circuits\ncircuits = CompositionAnalysis.detect_induction_circuit(capture)\n</code></pre>"},{"location":"getting-started/core-concepts/#4-intervene-optional","title":"4. Intervene (Optional)","text":"<pre><code>from oculi.intervention import SpectraScaler, InterventionContext\n\nscaler = SpectraScaler(layer=23, head=5, alpha=1.5)\n\nwith InterventionContext(adapter, [scaler]):\n    output = adapter.generate(prompt, max_new_tokens=10)\n</code></pre>"},{"location":"getting-started/core-concepts/#understanding-shapes","title":"Understanding Shapes","text":""},{"location":"getting-started/core-concepts/#attention-shapes","title":"Attention Shapes","text":"<pre><code>L  = n_layers       # Number of transformer layers\nH  = n_heads        # Number of query heads\nH_kv = n_kv_heads   # Number of key/value heads (GQA)\nT  = n_tokens       # Sequence length\nD  = head_dim       # Dimension per head\n</code></pre> <p>Common Shapes:</p> Tensor Shape Description <code>queries</code> <code>[L, H, T, D]</code> Query vectors per layer/head/position <code>keys</code> <code>[L, H_kv, T, D]</code> Key vectors (note H_kv for GQA) <code>values</code> <code>[L, H_kv, T, D]</code> Value vectors <code>patterns</code> <code>[L, H, T, T]</code> Attention weights (i\u2192j)"},{"location":"getting-started/core-concepts/#analysis-shapes","title":"Analysis Shapes","text":"Method Output Shape Description <code>token_entropy()</code> <code>[L, H, T]</code> Entropy per position <code>q_norms()</code> <code>[L, H, T]</code> Query norms <code>direct_logit_attribution()</code> <code>[L]</code> Layer contributions <code>component_attribution()</code> <code>[L, 2]</code> Attention vs MLP"},{"location":"getting-started/core-concepts/#grouped-query-attention-gqa","title":"Grouped Query Attention (GQA)","text":"<p>Modern LLMs use GQA where multiple query heads share key/value heads:</p> <pre><code># LLaMA-3-8B example\nn_heads = 32      # Query heads\nn_kv_heads = 8    # KV heads\ngqa_ratio = 4     # 4 query heads per KV head\n\n# Shapes reflect this\nqueries.shape  # [32, 32, T, 128]\nkeys.shape     # [32, 8, T, 128]  # Note: 8 not 32!\nvalues.shape   # [32, 8, T, 128]\n</code></pre> <p>Detection:</p> <pre><code>if capture.is_gqa:\n    ratio = capture.gqa_ratio\n    print(f\"GQA model: {ratio}:1 ratio\")\n</code></pre>"},{"location":"getting-started/core-concepts/#intervention-points","title":"Intervention Points","text":"<p>Oculi captures at strategic intervention points:</p> <pre><code>Input\n  \u2193\n[Layer 0]\n  pre_attn  \u2190\u2500 Capture point 1\n  \u2193\n  Attention\n  \u2193\n  post_attn \u2190\u2500 Capture point 2\n  \u2193\n  pre_mlp   \u2190\u2500 Capture point 3\n  \u2193\n  MLP\n  \u2193\n  post_mlp  \u2190\u2500 Capture point 4\n  \u2193\n[Layer 1]\n  ...\n</code></pre> <p>Each point reveals different aspects of computation.</p>"},{"location":"getting-started/core-concepts/#configuration-system","title":"Configuration System","text":""},{"location":"getting-started/core-concepts/#captureconfig","title":"CaptureConfig","text":"<pre><code>config = CaptureConfig(\n    layers=[20, 21, 22],        # Which layers\n    capture_queries=True,        # What components\n    capture_keys=True,\n    capture_values=True,\n    capture_patterns=True,\n    qk_stage='pre_rope'         # When (pre/post RoPE)\n)\n</code></pre>"},{"location":"getting-started/core-concepts/#residualconfig","title":"ResidualConfig","text":"<pre><code>config = ResidualConfig(\n    layers=[20, 21, 22],\n    capture_pre_attn=True,\n    capture_post_attn=True,\n    capture_pre_mlp=True,\n    capture_post_mlp=True\n)\n</code></pre>"},{"location":"getting-started/core-concepts/#logitconfig","title":"LogitConfig","text":"<pre><code>config = LogitConfig(\n    layers=None,  # All layers\n    top_k=10      # Memory-efficient: top-10 only\n)\n</code></pre>"},{"location":"getting-started/core-concepts/#testing-strategy","title":"Testing Strategy","text":""},{"location":"getting-started/core-concepts/#three-testing-tiers","title":"Three Testing Tiers","text":"<p>Contract Tests - Shape and semantic contracts: <pre><code>def test_entropy_shape():\n    entropy = EntropyAnalysis.token_entropy(capture)\n    assert entropy.shape == (L, H, T)\n</code></pre></p> <p>Integration Tests - End-to-end with mock models: <pre><code>def test_full_capture():\n    adapter = MockLlamaAdapter()\n    full = adapter.capture_full(input_ids)\n    assert full.attention is not None\n</code></pre></p> <p>Performance Tests - Real models (GPU): <pre><code>def test_memory_efficiency():\n    # Only run on GPU machines\n    ...\n</code></pre></p>"},{"location":"getting-started/core-concepts/#mock-models-for-cpu","title":"Mock Models for CPU","text":"<pre><code>from tests.mocks import MockLlamaAdapter\n\n# Tiny model for testing\nadapter = MockLlamaAdapter()\ninput_ids = adapter.tokenize(\"Test\")\ncapture = adapter.capture(input_ids)\n</code></pre>"},{"location":"getting-started/core-concepts/#best-practices","title":"Best Practices","text":""},{"location":"getting-started/core-concepts/#1-always-read-files-first","title":"1. Always Read Files First","text":"<pre><code># Before using an adapter\nfrom oculi.models.llama import LlamaAttentionAdapter\n\n# Check out the source code at:\n# oculi/models/llama/adapter.py\n# oculi/models/llama/anatomy.py\n</code></pre>"},{"location":"getting-started/core-concepts/#2-use-type-hints","title":"2. Use Type Hints","text":"<pre><code>from oculi.capture.structures import AttentionCapture\n\ndef my_analysis(capture: AttentionCapture) -&gt; torch.Tensor:\n    ...\n</code></pre>"},{"location":"getting-started/core-concepts/#3-validate-configs","title":"3. Validate Configs","text":"<pre><code>config = CaptureConfig(layers=[100])  # Invalid!\nconfig.validate(adapter)  # Raises ValueError\n</code></pre>"},{"location":"getting-started/core-concepts/#4-handle-gqa-properly","title":"4. Handle GQA Properly","text":"<pre><code># Account for GQA in analysis\nif capture.is_gqa:\n    # Keys/values have H_kv heads, not H heads\n    n_kv_heads = capture.n_kv_heads\n</code></pre>"},{"location":"getting-started/core-concepts/#5-memory-management","title":"5. Memory Management","text":"<pre><code># For large models\nconfig = CaptureConfig(\n    layers=[20, 21, 22],     # Subset only\n    capture_values=False     # Skip if not needed\n)\n</code></pre>"},{"location":"getting-started/core-concepts/#next-steps","title":"Next Steps","text":"<ul> <li> <p>Quick Start</p> <p>Get up and running</p> <p> Quick Start</p> </li> <li> <p>User Guides</p> <p>In-depth feature docs</p> <p> Guides</p> </li> <li> <p>API Reference</p> <p>Complete API docs</p> <p> API</p> </li> <li> <p>Examples</p> <p>Working code samples</p> <p> Examples</p> </li> </ul>"},{"location":"getting-started/installation/","title":"Installation","text":""},{"location":"getting-started/installation/#requirements","title":"Requirements","text":"<p>Oculi requires:</p> <ul> <li>Python: 3.10 or higher</li> <li>PyTorch: 2.0.0 or higher</li> <li>Transformers: 4.30.0 or higher</li> </ul>"},{"location":"getting-started/installation/#installation-methods","title":"Installation Methods","text":""},{"location":"getting-started/installation/#from-pypi-recommended","title":"From PyPI (Recommended)","text":"<p>Coming Soon</p> <p>PyPI distribution will be available with v1.0.0 release.</p>"},{"location":"getting-started/installation/#from-source","title":"From Source","text":"<p>Install directly from GitHub:</p> <pre><code># Clone the repository\ngit clone https://github.com/ajayspatil7/oculi.git\ncd oculi\n\n# Install in editable mode\npip install -e .\n</code></pre>"},{"location":"getting-started/installation/#optional-dependencies","title":"Optional Dependencies","text":"<p>Install additional features as needed:</p> <pre><code># Visualization support\npip install -e \".[viz]\"\n\n# Development tools (testing, linting)\npip install -e \".[dev]\"\n\n# Documentation tools\npip install -e \".[docs]\"\n\n# Everything\npip install -e \".[all]\"\n</code></pre>"},{"location":"getting-started/installation/#verify-installation","title":"Verify Installation","text":"<p>Verify your installation:</p> <pre><code>import oculi\nprint(f\"Oculi version: {oculi.__version__}\")\n\n# Test basic import\nfrom oculi.models.llama import LlamaAttentionAdapter\nfrom oculi.analysis import AttributionMethods, CompositionAnalysis\nprint(\"\u2713 All imports successful!\")\n</code></pre>"},{"location":"getting-started/installation/#platform-specific-notes","title":"Platform-Specific Notes","text":""},{"location":"getting-started/installation/#macos-apple-silicon","title":"macOS (Apple Silicon)","text":"<p>For Apple Silicon Macs (M1/M2/M3/M4):</p> <pre><code># Use MPS backend for acceleration\nexport PYTORCH_ENABLE_MPS_FALLBACK=1\n</code></pre> <p>Oculi works on CPU, but some operations may be slower. For best performance, use CUDA-enabled systems for large models.</p>"},{"location":"getting-started/installation/#linux-cuda","title":"Linux (CUDA)","text":"<p>Ensure you have CUDA-compatible PyTorch:</p> <pre><code># Check PyTorch CUDA availability\npython -c \"import torch; print(f'CUDA available: {torch.cuda.is_available()}')\"\n</code></pre>"},{"location":"getting-started/installation/#windows","title":"Windows","text":"<p>Oculi supports Windows with standard Python installation. Use WSL2 for best compatibility with CUDA.</p>"},{"location":"getting-started/installation/#testing-without-gpu","title":"Testing Without GPU","text":"<p>Oculi includes mock models for testing on CPU:</p> <pre><code>from tests.mocks import MockLlamaAdapter\n\n# Tiny mock model for testing\nadapter = MockLlamaAdapter()\ninput_ids = adapter.tokenize(\"Test input\")\n\n# All features work\ncapture = adapter.capture(input_ids)\nprint(f\"\u2713 Capture successful: {capture.patterns.shape}\")\n</code></pre>"},{"location":"getting-started/installation/#troubleshooting","title":"Troubleshooting","text":""},{"location":"getting-started/installation/#importerror-no-module-named-oculi","title":"ImportError: No module named 'oculi'","text":"<p>Ensure you installed in editable mode with <code>-e</code> flag:</p> <pre><code>pip install -e .\n</code></pre>"},{"location":"getting-started/installation/#cuda-out-of-memory","title":"CUDA out of memory","text":"<p>For large models, use selective capture:</p> <pre><code>from oculi import CaptureConfig\n\nconfig = CaptureConfig(\n    layers=[20, 21, 22],  # Only specific layers\n    capture_values=False   # Skip values if not needed\n)\n\ncapture = adapter.capture(input_ids, config=config)\n</code></pre>"},{"location":"getting-started/installation/#version-conflicts","title":"Version conflicts","text":"<p>Check your environment:</p> <pre><code>pip list | grep -E \"torch|transformers|oculi\"\n</code></pre> <p>Ensure compatible versions: - PyTorch &gt;= 2.0.0 - Transformers &gt;= 4.30.0</p>"},{"location":"getting-started/installation/#next-steps","title":"Next Steps","text":"<ul> <li>Quick Start Guide - Get started in 5 minutes</li> <li>Core Concepts - Understand key abstractions</li> <li>User Guides - In-depth feature documentation</li> </ul>"},{"location":"getting-started/quick-start/","title":"Quick Start","text":"<p>Get started with Oculi in 5 minutes.</p>"},{"location":"getting-started/quick-start/#installation","title":"Installation","text":"<pre><code>git clone https://github.com/ajayspatil7/oculi.git\ncd oculi\npip install -e .\n</code></pre>"},{"location":"getting-started/quick-start/#basic-usage","title":"Basic Usage","text":""},{"location":"getting-started/quick-start/#1-load-a-model","title":"1. Load a Model","text":"<pre><code>from transformers import AutoModelForCausalLM, AutoTokenizer\nfrom oculi.models.llama import LlamaAttentionAdapter\n\n# Load model (works with any LLaMA 2/3 model)\nmodel = AutoModelForCausalLM.from_pretrained(\"meta-llama/Meta-Llama-3-8B\")\ntokenizer = AutoTokenizer.from_pretrained(\"meta-llama/Meta-Llama-3-8B\")\n\n# Create Oculi adapter\nadapter = LlamaAttentionAdapter(model, tokenizer)\n</code></pre>"},{"location":"getting-started/quick-start/#2-capture-attention","title":"2. Capture Attention","text":"<pre><code># Prepare input\ntext = \"The quick brown fox jumps over the lazy dog\"\ninput_ids = tokenizer.encode(text, return_tensors=\"pt\")\n\n# Capture attention data\ncapture = adapter.capture(input_ids)\n\nprint(f\"Queries: {capture.queries.shape}\")   # [L, H, T, D]\nprint(f\"Keys: {capture.keys.shape}\")         # [L, H_kv, T, D]\nprint(f\"Values: {capture.values.shape}\")     # [L, H_kv, T, D]\nprint(f\"Patterns: {capture.patterns.shape}\") # [L, H, T, T]\n</code></pre> <p>Output: <pre><code>Queries: torch.Size([32, 32, 10, 128])\nKeys: torch.Size([32, 8, 10, 128])\nValues: torch.Size([32, 8, 10, 128])\nPatterns: torch.Size([32, 32, 10, 10])\n</code></pre></p>"},{"location":"getting-started/quick-start/#3-analyze-attention","title":"3. Analyze Attention","text":"<pre><code>from oculi.analysis import EntropyAnalysis, CircuitDetection\n\n# Compute entropy (how diffuse is attention?)\nentropy = EntropyAnalysis.token_entropy(capture)\nprint(f\"Entropy shape: {entropy.shape}\")  # [L, H, T]\n\n# Detect induction heads\ninduction_heads = CircuitDetection.detect_induction_heads(capture)\nprint(f\"Induction heads detected: {induction_heads.sum()}\")\n</code></pre>"},{"location":"getting-started/quick-start/#4-full-capture-everything","title":"4. Full Capture (Everything)","text":"<pre><code># Capture attention + residual + MLP + logits in one pass\nfull = adapter.capture_full(input_ids)\n\nprint(f\"Attention: {full.attention is not None}\")  # True\nprint(f\"Residual: {full.residual is not None}\")    # True\nprint(f\"MLP: {full.mlp is not None}\")              # True\nprint(f\"Logits: {full.logits is not None}\")        # True\n</code></pre>"},{"location":"getting-started/quick-start/#phase-2-features-new","title":"Phase 2 Features (NEW)","text":""},{"location":"getting-started/quick-start/#attribution-analysis","title":"Attribution Analysis","text":"<pre><code>from oculi.analysis import AttributionMethods\n\n# Which layers contribute most to the output?\ntarget_token_id = tokenizer.encode(\"dog\")[0]\nunembed = model.lm_head.weight\n\nattribution = AttributionMethods.direct_logit_attribution(\n    full.residual, unembed, target_token_id\n)\nprint(f\"Most important layer: {attribution.values.argmax().item()}\")\n</code></pre>"},{"location":"getting-started/quick-start/#composition-analysis","title":"Composition Analysis","text":"<pre><code>from oculi.analysis import CompositionAnalysis\n\n# How do heads compose?\nqk_comp = CompositionAnalysis.qk_composition(\n    full.attention,\n    source=(10, 5),  # Layer 10, Head 5\n    target=(20, 3)   # Layer 20, Head 3\n)\nprint(f\"Composition score: {qk_comp.values.mean():.4f}\")\n\n# Detect induction circuits\ncircuits = CompositionAnalysis.detect_induction_circuit(full.attention)\nprint(f\"Found {len(circuits.metadata['circuits'])} induction circuits\")\n</code></pre>"},{"location":"getting-started/quick-start/#common-patterns","title":"Common Patterns","text":""},{"location":"getting-started/quick-start/#selective-capture-save-memory","title":"Selective Capture (Save Memory)","text":"<pre><code>from oculi import CaptureConfig\n\n# Only capture specific layers and components\nconfig = CaptureConfig(\n    layers=[20, 21, 22],       # Last 3 layers only\n    capture_queries=True,\n    capture_keys=False,        # Don't need keys\n    capture_values=False,      # Don't need values\n    capture_patterns=True\n)\n\ncapture = adapter.capture(input_ids, config=config)\n</code></pre>"},{"location":"getting-started/quick-start/#intervention-example","title":"Intervention Example","text":"<pre><code>from oculi.intervention import SpectraScaler, InterventionContext\n\n# Sharpen attention at layer 23, head 5\nscaler = SpectraScaler(layer=23, head=5, alpha=1.5)\n\nwith InterventionContext(adapter, [scaler]):\n    output = adapter.generate(\n        \"The capital of France is\",\n        max_new_tokens=10\n    )\nprint(output)\n</code></pre>"},{"location":"getting-started/quick-start/#testing-without-gpu","title":"Testing Without GPU","text":"<p>Use mock models for quick testing:</p> <pre><code>from tests.mocks import MockLlamaAdapter\n\n# Tiny model for CPU testing\nadapter = MockLlamaAdapter()\ninput_ids = adapter.tokenize(\"Test input\")\n\n# All features work\ncapture = adapter.capture(input_ids)\nfull = adapter.capture_full(input_ids)\n</code></pre>"},{"location":"getting-started/quick-start/#next-steps","title":"Next Steps","text":"<ul> <li> <p>Learn Core Concepts</p> <p>Understand Oculi's design philosophy</p> <p> Core Concepts</p> </li> <li> <p>Explore Guides</p> <p>In-depth documentation for each feature</p> <p> User Guides</p> </li> <li> <p>Try Tutorials</p> <p>Step-by-step examples with explanations</p> <p> Tutorials</p> </li> <li> <p>API Reference</p> <p>Complete API documentation</p> <p> API Docs</p> </li> </ul>"},{"location":"getting-started/quick-start/#help-support","title":"Help &amp; Support","text":"<ul> <li>Issues: GitHub Issues</li> <li>Discussions: GitHub Discussions</li> <li>Contributing: Contributing Guide</li> </ul>"},{"location":"guides/attribution-methods/","title":"Attribution Methods","text":"<p>Attribution methods help you understand how information flows through the transformer and which components contribute to specific outputs.</p>"},{"location":"guides/attribution-methods/#overview","title":"Overview","text":"<p>Oculi provides five attribution methods:</p> <ol> <li>Attention Flow - Track information propagation through attention layers</li> <li>Value-Weighted Attention - Account for value vector magnitudes</li> <li>Direct Logit Attribution - Measure layer contributions to target logits</li> <li>Component Attribution - Decompose into attention vs MLP contributions</li> <li>Head Attribution - Per-head contribution to target logits</li> </ol>"},{"location":"guides/attribution-methods/#prerequisites","title":"Prerequisites","text":"<pre><code>from transformers import AutoModelForCausalLM, AutoTokenizer\nfrom oculi.models.llama import LlamaAttentionAdapter\nfrom oculi.analysis import AttributionMethods\n\n# Load model\nmodel = AutoModelForCausalLM.from_pretrained(\"meta-llama/Meta-Llama-3-8B\")\ntokenizer = AutoTokenizer.from_pretrained(\"meta-llama/Meta-Llama-3-8B\")\nadapter = LlamaAttentionAdapter(model, tokenizer)\n\n# Capture full model state\ntext = \"The cat sat on the mat\"\ninput_ids = tokenizer.encode(text, return_tensors=\"pt\")\nfull = adapter.capture_full(input_ids)\n</code></pre>"},{"location":"guides/attribution-methods/#attention-flow","title":"Attention Flow","text":"<p>Purpose: Track how information from source positions propagates to target positions through attention across layers.</p> <p>Method: Compose attention patterns across layers via matrix multiplication.</p> <pre><code>flow = AttributionMethods.attention_flow(full.attention, normalize=True)\n\nprint(f\"Flow shape: {flow.values.shape}\")  # [L, H, T, T]\n\n# flow.values[l, h, i, j] = cumulative attention from position j to i at layer l\n# This tells you: \"How much does token j contribute to token i's representation at layer l?\"\n\n# Example: Track how token 0 (BOS) contributes across layers\nlayer = 20\nhead = 5\ntoken_from = 0\ntoken_to = -1  # Last token\n\ncontribution = flow.values[layer, head, token_to, token_from]\nprint(f\"BOS contribution to final token at L{layer}H{head}: {contribution:.4f}\")\n</code></pre> <p>Interpretation: - High flow value \u2192 Strong cumulative attention path - Low flow value \u2192 Weak or indirect attention path - Diagonal values \u2192 Self-attention strength</p>"},{"location":"guides/attribution-methods/#value-weighted-attention","title":"Value-Weighted Attention","text":"<p>Purpose: Correct attention patterns by accounting for value vector magnitudes. High attention doesn't always mean high contribution if values are small.</p> <pre><code>weighted_attn = AttributionMethods.value_weighted_attention(\n    full.attention,\n    norm_type=\"l2\"  # Options: \"l2\", \"l1\", \"linf\"\n)\n\nprint(f\"Weighted attention: {weighted_attn.values.shape}\")  # [L, H, T, T]\n\n# Compare raw vs value-weighted attention\nlayer, head, token = 20, 5, -1\nraw_attn = full.attention.patterns[layer, head, token, :]\nweighted = weighted_attn.values[layer, head, token, :]\n\nprint(f\"Raw attention sum: {raw_attn.sum():.4f}\")  # Should be ~1.0\nprint(f\"Weighted attention sum: {weighted.sum():.4f}\")  # Should be ~1.0\nprint(f\"Difference: {(weighted - raw_attn).abs().sum():.4f}\")\n</code></pre> <p>Use Cases: - Identifying truly important tokens (high attention \u00d7 high value magnitude) - Detecting \"attention sinks\" (high attention but low contribution) - Value norm analysis by attention head</p>"},{"location":"guides/attribution-methods/#direct-logit-attribution","title":"Direct Logit Attribution","text":"<p>Purpose: Measure how much each layer's residual stream contribution affects a specific target token's logit.</p> <pre><code># Which layers are most important for predicting \"mat\"?\ntarget_token_id = tokenizer.encode(\"mat\")[0]\nunembed = model.lm_head.weight\n\ndla = AttributionMethods.direct_logit_attribution(\n    full.residual,\n    unembed,\n    target_token_id,\n    position=-1  # Last position\n)\n\nprint(f\"Layer attributions: {dla.values.shape}\")  # [L]\n\n# Find most important layers\ntop_layers = dla.values.abs().topk(5)\nfor i, (score, layer) in enumerate(zip(top_layers.values, top_layers.indices)):\n    print(f\"{i+1}. Layer {layer.item()}: {score.item():.4f}\")\n</code></pre> <p>Interpretation: - Positive values \u2192 Layer increases target logit - Negative values \u2192 Layer decreases target logit - Large magnitude \u2192 Strong causal effect - Small magnitude \u2192 Weak or negligible effect</p> <p>Typical Patterns: - Early layers often have small contributions - Middle layers build up features - Late layers make final adjustments</p>"},{"location":"guides/attribution-methods/#component-attribution","title":"Component Attribution","text":"<p>Purpose: Decompose each layer's contribution into attention vs MLP components.</p> <pre><code>component_attr = AttributionMethods.component_attribution(\n    full.residual,\n    full.mlp,\n    unembed,\n    target_token_id,\n    position=-1\n)\n\nprint(f\"Component attribution: {component_attr.values.shape}\")  # [L, 2]\n\n# Extract attention and MLP contributions\nattn_contrib = component_attr.values[:, 0]\nmlp_contrib = component_attr.values[:, 1]\n\n# Analyze contribution patterns\nfor layer in range(len(attn_contrib)):\n    a = attn_contrib[layer].item()\n    m = mlp_contrib[layer].item()\n    dominant = \"Attention\" if abs(a) &gt; abs(m) else \"MLP\"\n    print(f\"Layer {layer:2d}: Attn={a:+.3f}, MLP={m:+.3f} ({dominant})\")\n</code></pre> <p>Use Cases: - Understanding attention vs MLP roles per layer - Identifying \"attention-heavy\" vs \"MLP-heavy\" layers - Debugging why certain layers matter - Circuit analysis (which components are involved)</p>"},{"location":"guides/attribution-methods/#head-attribution","title":"Head Attribution","text":"<p>Purpose: Granular per-head attribution to target logits.</p> <pre><code># Need output projection weights\noutput_weights = model.model.layers[0].self_attn.o_proj.weight\n\nhead_attr = AttributionMethods.head_attribution(\n    full.attention,\n    output_weights,\n    unembed,\n    target_token_id,\n    position=-1\n)\n\nprint(f\"Head attribution: {head_attr.values.shape}\")  # [L, H]\n\n# Get top contributing heads\ntop_heads = AttributionMethods.top_attributions(head_attr, k=10)\nfor (layer, head), score in top_heads:\n    print(f\"L{layer}H{head}: {score:.4f}\")\n</code></pre> <p>Note: Head attribution requires model's output projection weights. Implementation may vary by model architecture.</p>"},{"location":"guides/attribution-methods/#combining-methods","title":"Combining Methods","text":"<p>Use multiple attribution methods together for comprehensive analysis:</p> <pre><code># 1. Find important layers\ndla = AttributionMethods.direct_logit_attribution(...)\nimportant_layers = dla.values.abs().topk(3).indices\n\n# 2. Decompose those layers into components\nfor layer in important_layers:\n    component_attr = AttributionMethods.component_attribution(...)\n    attn_score = component_attr.values[layer, 0]\n    mlp_score = component_attr.values[layer, 1]\n    print(f\"Layer {layer}: Attn={attn_score:.3f}, MLP={mlp_score:.3f}\")\n\n# 3. If attention dominates, look at head-level\nif attn_score &gt; mlp_score:\n    head_attr = AttributionMethods.head_attribution(...)\n    layer_heads = head_attr.values[layer, :]\n    top_head = layer_heads.argmax()\n    print(f\"  \u2192 Dominated by Head {top_head}\")\n</code></pre>"},{"location":"guides/attribution-methods/#top-attributions-helper","title":"Top Attributions Helper","text":"<p>Extract top-k attributions with indices:</p> <pre><code>result = AttributionMethods.direct_logit_attribution(...)\n\n# Get top 10 by absolute value\ntop_10 = AttributionMethods.top_attributions(result, k=10)\n\nfor indices, value in top_10:\n    print(f\"Index {indices}: {value:.4f}\")\n</code></pre> <p>Works with any <code>AttributionResult</code> regardless of shape.</p>"},{"location":"guides/attribution-methods/#mathematical-background","title":"Mathematical Background","text":""},{"location":"guides/attribution-methods/#attention-flow_1","title":"Attention Flow","text":"<pre><code>flow[0] = attn[0]  # Layer 0: direct attention\nflow[l] = attn[l] @ flow[l-1]  # Subsequent layers: composition\n</code></pre>"},{"location":"guides/attribution-methods/#value-weighted-attention_1","title":"Value-Weighted Attention","text":"<pre><code>weighted[t_q, t_k] = attn[t_q, t_k] * ||V[t_k]||\n</code></pre>"},{"location":"guides/attribution-methods/#direct-logit-attribution_1","title":"Direct Logit Attribution","text":"<pre><code>delta[L] = residual[L] - residual[L-1]\nattribution[L] = delta[L] @ unembed[target_token]\n</code></pre>"},{"location":"guides/attribution-methods/#component-attribution_1","title":"Component Attribution","text":"<pre><code>attn_delta[L] = post_attn[L] - pre_attn[L]\nmlp_delta[L] = post_mlp[L] - pre_mlp[L]\nattribution = {attn_delta, mlp_delta} @ unembed[target]\n</code></pre>"},{"location":"guides/attribution-methods/#best-practices","title":"Best Practices","text":"<ol> <li> <p>Always capture full state for attribution:    <pre><code>full = adapter.capture_full(input_ids)\n</code></pre></p> </li> <li> <p>Use absolute values for importance ranking:    <pre><code>important = attribution.values.abs().topk(k)\n</code></pre></p> </li> <li> <p>Normalize for comparison:    <pre><code>normalized = attribution.values / attribution.values.abs().sum()\n</code></pre></p> </li> <li> <p>Combine multiple methods for complete picture</p> </li> <li> <p>Visualize results for interpretation</p> </li> </ol>"},{"location":"guides/attribution-methods/#next-steps","title":"Next Steps","text":"<ul> <li>Composition Analysis - Analyze head interactions</li> <li>Logit Lens - Track predictions across layers</li> <li>Circuit Detection - Identify canonical circuits</li> <li>API Reference - Complete API documentation</li> </ul>"}]}